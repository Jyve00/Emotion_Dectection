{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch \n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/karthik999/pytorch-human-speech-classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Dataframe with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Actor_24'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modality = {'01':'full_av','02':'video_only','03':'audio_only'}\n",
    "vocal_channel = {'01':'speech','02':'song'}\n",
    "emotion = {'01':'neutral','02':'calm','03':'happy','04':'sad','05':'angry','06':'fearful','07':'disgust','08':'surprised'}\n",
    "emotional_intensity = {'01':'normal','02':'strong'}\n",
    "statement = {'01':'Kids are talking by the door','02':'Dogs are sitting by the door'}\n",
    "reptition = {'01':'first_repitition','02':'second_repetition'}\n",
    "def actor_f(num):\n",
    "    if int(num)%2==0: return('female')\n",
    "    else: return('male')\n",
    "\n",
    "actors = sorted(os.listdir('/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24'))\n",
    "actors.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_dict = {}\n",
    "for actor in actors:\n",
    "    actor_dir = os.path.join('/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24',actor)\n",
    "    actor_files = os.listdir(actor_dir)\n",
    "    actor_dict = [i.replace(\".wav\",\"\").split(\"-\") for i in actor_files]\n",
    "    dict_entry = {os.path.join(actor_dir,i):j for i,j in zip(actor_files,actor_dict)}\n",
    "    audio_file_dict.update(dict_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modality</th>\n",
       "      <th>vocal_channel</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotional_intensity</th>\n",
       "      <th>statement</th>\n",
       "      <th>repetition</th>\n",
       "      <th>actor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-08-02-02-01-01.wav</th>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>08</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-08-01-01-01-01.wav</th>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>08</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-05-01-02-01-01.wav</th>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>05</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-06-01-02-02-01.wav</th>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>06</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-06-02-01-02-01.wav</th>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>06</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-03-02-02-02-23.wav</th>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>03</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-03-01-01-02-23.wav</th>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-02-02-01-01-23.wav</th>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-02-01-02-01-23.wav</th>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>01</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-01-01-02-02-23.wav</th>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1380 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   modality vocal_channel  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       03            01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       03            01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       03            01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       03            01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       03            01   \n",
       "...                                                     ...           ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       03            01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       03            01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       03            01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       03            01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       03            01   \n",
       "\n",
       "                                                   emotion  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      08   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      08   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      05   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      06   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      06   \n",
       "...                                                    ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      03   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      03   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      02   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      02   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      01   \n",
       "\n",
       "                                                   emotional_intensity  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...                  02   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...                  01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...                  01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...                  01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...                  02   \n",
       "...                                                                ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...                  02   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...                  01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...                  02   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...                  01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...                  01   \n",
       "\n",
       "                                                   statement repetition actor  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...        02         01    01  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...        01         01    01  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...        02         01    01  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...        02         02    01  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...        01         02    01  \n",
       "...                                                      ...        ...   ...  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...        02         02    23  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...        01         02    23  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...        01         01    23  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...        02         01    23  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...        02         02    23  \n",
       "\n",
       "[1380 rows x 7 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file_dict = pd.DataFrame(audio_file_dict).T\n",
    "audio_file_dict.columns = ['modality','vocal_channel','emotion','emotional_intensity','statement','repetition','actor']\n",
    "audio_file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modality</th>\n",
       "      <th>vocal_channel</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotional_intensity</th>\n",
       "      <th>statement</th>\n",
       "      <th>repetition</th>\n",
       "      <th>actor</th>\n",
       "      <th>actor_sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-08-02-02-01-01.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>surprised</td>\n",
       "      <td>strong</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>first_repitition</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-08-01-01-01-01.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>surprised</td>\n",
       "      <td>normal</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>first_repitition</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-05-01-02-01-01.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>angry</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>first_repitition</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-06-01-02-02-01.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>fearful</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>second_repetition</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-06-02-01-02-01.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>fearful</td>\n",
       "      <td>strong</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>second_repetition</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-03-02-02-02-23.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>happy</td>\n",
       "      <td>strong</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>second_repetition</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-03-01-01-02-23.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>happy</td>\n",
       "      <td>normal</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>second_repetition</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-02-02-01-01-23.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>calm</td>\n",
       "      <td>strong</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>first_repitition</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-02-01-02-01-23.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>calm</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>first_repitition</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-01-01-02-02-23.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>neutral</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>second_repetition</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1380 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      modality vocal_channel  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "...                                                        ...           ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "\n",
       "                                                      emotion  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  surprised   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  surprised   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      angry   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...    fearful   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...    fearful   \n",
       "...                                                       ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      happy   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      happy   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       calm   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       calm   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...    neutral   \n",
       "\n",
       "                                                   emotional_intensity  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              strong   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              normal   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              normal   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              normal   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              strong   \n",
       "...                                                                ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              strong   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              normal   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              strong   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              normal   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              normal   \n",
       "\n",
       "                                                                       statement  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Dogs are sitting by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Kids are talking by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Dogs are sitting by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Dogs are sitting by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Kids are talking by the door   \n",
       "...                                                                          ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Dogs are sitting by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Kids are talking by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Kids are talking by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Dogs are sitting by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Dogs are sitting by the door   \n",
       "\n",
       "                                                           repetition actor  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...   first_repitition    01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...   first_repitition    01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...   first_repitition    01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  second_repetition    01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  second_repetition    01   \n",
       "...                                                               ...   ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  second_repetition    23   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  second_repetition    23   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...   first_repitition    23   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...   first_repitition    23   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  second_repetition    23   \n",
       "\n",
       "                                                   actor_sex  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "...                                                      ...  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "\n",
       "[1380 rows x 8 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file_dict.modality = audio_file_dict.modality.map(modality)\n",
    "audio_file_dict.vocal_channel = audio_file_dict.vocal_channel.map(vocal_channel)\n",
    "audio_file_dict.emotion = audio_file_dict.emotion.map(emotion)\n",
    "audio_file_dict.emotional_intensity = audio_file_dict.emotional_intensity.map(emotional_intensity)\n",
    "audio_file_dict.statement = audio_file_dict.statement.map(statement)\n",
    "audio_file_dict.repetition = audio_file_dict.repetition.map(reptition)\n",
    "audio_file_dict['actor_sex'] = audio_file_dict.actor.apply(actor_f)\n",
    "\n",
    "audio_file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all audio files the same shape \n",
    "\n",
    "audio_files = []\n",
    "for i in list(audio_file_dict.index):\n",
    "    i, _ = torchaudio.load(i)\n",
    "    audio_files.append(i)\n",
    "\n",
    "maxlen = 0\n",
    "minlen = np.Inf\n",
    "for i in audio_files:\n",
    "    if i.shape[1]>maxlen:\n",
    "        maxlen = i.shape[1]\n",
    "    if i.shape[1]<minlen:\n",
    "        minlen = i.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140941, 253053)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minlen, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms = []\n",
    "for i in audio_files:\n",
    "    specgram = torchaudio.transforms.Spectrogram()(i)\n",
    "    spectrograms.append(specgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_width, max_height = max([i.shape[2] for i in spectrograms]), max([i.shape[1] for i in spectrograms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = [\n",
    "    # The needed padding is the difference between the\n",
    "    # max width/height and the image's actual width/height.\n",
    "    F.pad(img, [0, max_width - img.size(2), 0, max_height - img.size(1)])\n",
    "    for img in spectrograms\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = torch.cat(image_batch,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "del audio_files, spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom dataset with Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_file_dict):\n",
    "        self.audio_file_dict = audio_file_dict.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        count = len(self.audio_file_dict)\n",
    "        return count\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            path = self.audio_file_dict.reset_index()['index'][index]\n",
    "            signal, sr = torchaudio.load(path)\n",
    "            signal = torch.mean(signal, dim=0).unsqueeze(0)\n",
    "            img = torchaudio.transforms.Spectrogram()(img)\n",
    "            img = F.pad(img, [0, max_width - img.size(2), 0, max_height - img.size(1)])\n",
    "            label = list(pd.get_dummies(self.audio_file_dict.reset_index().emotion).iloc[index].values)\n",
    "            label = np.array(label)\n",
    "            label = torch.from_numpy(label)\n",
    "            return (img, label)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966 414 966 414 1380\n",
      "194 83\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "num_classes = 8\n",
    "batch_size = 5\n",
    "learning_rate = 0.000001\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test = train_test_split(audio_file_dict, test_size = 0.3)\n",
    "\n",
    "train_data = EmotionDataset(audio_file_dict=X_train)\n",
    "test_data = EmotionDataset(audio_file_dict=X_test)\n",
    "\n",
    "print(len(X_train), len(X_test), len(train_data), len(test_data), len(audio_file_dict))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(len(train_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(242688, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 8)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=242688, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3727, -1.2635, -2.5786, -1.2426, -2.3086],\n",
      "        [-3.2219, -1.5212, -3.5008, -0.9936, -1.0751],\n",
      "        [-1.8226, -1.7957, -3.4201, -2.3477, -0.6087]],\n",
      "       grad_fn=<LogSoftmaxBackward0>) torch.Size([3, 5]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "\n",
    "print(m(input), m(input).shape, target.shape)\n",
    "output = loss(m(input), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> tensor([4, 4, 4, 5, 2]) torch.Size([5]) tensor([4, 4, 4, 5, 2]) tensor([1, 1, 2, 2, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m3/ywsrz7k170vbk8511l1h_6840000gn/T/ipykernel_60905/805867866.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Track the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Run the forward pass\n",
    "        images = images\n",
    "        labels = torch.max(labels, 1)[1]\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(\">>\", predicted, predicted.shape, predicted, labels)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "    print(f'epoch: {epoch}: acc:',np.mean(acc_list),'loss: ',np.mean(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3358073929bd3d27ff594bc6528257efc4213b34ba9d7c6bd240dce3a23a83d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('torch_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
