{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simple Model \n",
    "Here we'll do a simple model using Librosa for audio feature extraction and Tensorflow for a Convolution Neural Network to predict human emotions with recorded speech data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "import numpy as np #? \n",
    "import pandas as pd \n",
    "import os \n",
    "import json \n",
    "import math \n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import librosa \n",
    "import librosa.display\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Importing required libraries \n",
    "# Keras\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.models import Sequential, Model, model_from_json\n",
    "#from keras.layers import Dense, Embedding, LSTM\n",
    "#from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "#from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "#from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.random import set_seed\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from tensorflow.keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                               Path\n",
       "0    angry  /Users/stephen/Emotion_Dectection/data/RAVDESS...\n",
       "1     fear  /Users/stephen/Emotion_Dectection/data/RAVDESS...\n",
       "2     fear  /Users/stephen/Emotion_Dectection/data/RAVDESS...\n",
       "3    angry  /Users/stephen/Emotion_Dectection/data/RAVDESS...\n",
       "4  disgust  /Users/stephen/Emotion_Dectection/data/RAVDESS..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import csv with metadata \n",
    "ref  = pd.read_csv('/Users/stephen/Emotion_Dectection/data/RAVDESS/Ravdess.csv',index_col=[0])\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1440 entries, 0 to 1439\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Emotions  1440 non-null   object\n",
      " 1   Path      1440 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 33.8+ KB\n"
     ]
    }
   ],
   "source": [
    "ref.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-56.372177, -56.372177, -56.372177, -56.37217...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-54.18071, -54.18071, -54.18071, -54.18071, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-43.63373, -43.63373, -43.63373, -43.63373, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-44.896935, -44.896935, -44.896935, -44.89693...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-63.29453, -63.29453, -63.29453, -63.29453, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature\n",
       "0  [-56.372177, -56.372177, -56.372177, -56.37217...\n",
       "1  [-54.18071, -54.18071, -54.18071, -54.18071, -...\n",
       "2  [-43.63373, -43.63373, -43.63373, -43.63373, -...\n",
       "3  [-44.896935, -44.896935, -44.896935, -44.89693...\n",
       "4  [-63.29453, -63.29453, -63.29453, -63.29453, -..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['feature'])\n",
    "\n",
    "counter = 0\n",
    "for index, path in enumerate(ref.Path): \n",
    "    X, sample_rate = librosa.load(path, sr=16000)\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
    "    df.loc[counter] = [mfccs]\n",
    "    counter=counter+1\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>-43.633732</td>\n",
       "      <td>-43.633732</td>\n",
       "      <td>-43.633732</td>\n",
       "      <td>-43.633732</td>\n",
       "      <td>-43.633732</td>\n",
       "      <td>-43.432880</td>\n",
       "      <td>-43.057529</td>\n",
       "      <td>-42.840691</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                               Path          0  \\\n",
       "0    angry  /Users/stephen/Emotion_Dectection/data/RAVDESS... -56.372177   \n",
       "1     fear  /Users/stephen/Emotion_Dectection/data/RAVDESS... -54.180710   \n",
       "2     fear  /Users/stephen/Emotion_Dectection/data/RAVDESS... -43.633732   \n",
       "3    angry  /Users/stephen/Emotion_Dectection/data/RAVDESS... -44.896935   \n",
       "4  disgust  /Users/stephen/Emotion_Dectection/data/RAVDESS... -63.294529   \n",
       "\n",
       "           1          2          3          4          5          6  \\\n",
       "0 -56.372177 -56.372177 -56.372177 -56.372177 -56.372177 -56.372177   \n",
       "1 -54.180710 -54.180710 -54.180710 -54.180710 -54.180710 -54.180710   \n",
       "2 -43.633732 -43.633732 -43.633732 -43.633732 -43.432880 -43.057529   \n",
       "3 -44.896935 -44.896935 -44.896935 -44.896935 -44.896935 -44.896935   \n",
       "4 -63.294529 -63.294529 -63.294529 -63.294529 -63.294529 -63.294529   \n",
       "\n",
       "           7  ...  155  156  157  158  159  160  161  162  163  164  \n",
       "0 -56.372177  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1 -54.180710  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2 -42.840691  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3 -44.896935  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4 -63.294529  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now extract the mean bands to its own feature columns\n",
    "df = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 167)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>-56.372177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>-54.180710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>-43.633732</td>\n",
       "      <td>-43.633732</td>\n",
       "      <td>-43.633732</td>\n",
       "      <td>-43.633732</td>\n",
       "      <td>-43.633732</td>\n",
       "      <td>-43.432880</td>\n",
       "      <td>-43.057529</td>\n",
       "      <td>-42.840691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>-44.896935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>-63.294529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                               Path          0  \\\n",
       "0    angry  /Users/stephen/Emotion_Dectection/data/RAVDESS... -56.372177   \n",
       "1     fear  /Users/stephen/Emotion_Dectection/data/RAVDESS... -54.180710   \n",
       "2     fear  /Users/stephen/Emotion_Dectection/data/RAVDESS... -43.633732   \n",
       "3    angry  /Users/stephen/Emotion_Dectection/data/RAVDESS... -44.896935   \n",
       "4  disgust  /Users/stephen/Emotion_Dectection/data/RAVDESS... -63.294529   \n",
       "\n",
       "           1          2          3          4          5          6  \\\n",
       "0 -56.372177 -56.372177 -56.372177 -56.372177 -56.372177 -56.372177   \n",
       "1 -54.180710 -54.180710 -54.180710 -54.180710 -54.180710 -54.180710   \n",
       "2 -43.633732 -43.633732 -43.633732 -43.633732 -43.432880 -43.057529   \n",
       "3 -44.896935 -44.896935 -44.896935 -44.896935 -44.896935 -44.896935   \n",
       "4 -63.294529 -63.294529 -63.294529 -63.294529 -63.294529 -63.294529   \n",
       "\n",
       "           7  ...  155  156  157  158  159  160  161  162  163  164  \n",
       "0 -56.372177  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1 -54.180710  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2 -42.840691  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3 -44.896935  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4 -63.294529  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace NA with 0\n",
    "df=df.fillna(0)\n",
    "print(df.shape)\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-59.765537</td>\n",
       "      <td>-59.535393</td>\n",
       "      <td>-58.336723</td>\n",
       "      <td>-59.388527</td>\n",
       "      <td>-59.765537</td>\n",
       "      <td>-58.550285</td>\n",
       "      <td>-59.391659</td>\n",
       "      <td>-59.765537</td>\n",
       "      <td>-59.701347</td>\n",
       "      <td>-58.975872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>-64.228317</td>\n",
       "      <td>-64.228317</td>\n",
       "      <td>-64.228317</td>\n",
       "      <td>-64.228317</td>\n",
       "      <td>-64.228317</td>\n",
       "      <td>-64.228317</td>\n",
       "      <td>-64.228317</td>\n",
       "      <td>-64.228317</td>\n",
       "      <td>-64.228317</td>\n",
       "      <td>-64.282455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>-48.397202</td>\n",
       "      <td>-48.397202</td>\n",
       "      <td>-48.397202</td>\n",
       "      <td>-48.397202</td>\n",
       "      <td>-48.397202</td>\n",
       "      <td>-48.397202</td>\n",
       "      <td>-48.397202</td>\n",
       "      <td>-48.397202</td>\n",
       "      <td>-48.397202</td>\n",
       "      <td>-48.397202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>-60.439041</td>\n",
       "      <td>-58.920818</td>\n",
       "      <td>-57.311176</td>\n",
       "      <td>-57.387520</td>\n",
       "      <td>-57.312222</td>\n",
       "      <td>-55.221611</td>\n",
       "      <td>-54.327877</td>\n",
       "      <td>-56.239277</td>\n",
       "      <td>-58.191216</td>\n",
       "      <td>-59.415623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>-55.609592</td>\n",
       "      <td>-56.180172</td>\n",
       "      <td>-56.180172</td>\n",
       "      <td>-55.646084</td>\n",
       "      <td>-55.201511</td>\n",
       "      <td>-56.180172</td>\n",
       "      <td>-56.180172</td>\n",
       "      <td>-56.180172</td>\n",
       "      <td>-56.180172</td>\n",
       "      <td>-56.180172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>-57.752029</td>\n",
       "      <td>-59.759384</td>\n",
       "      <td>-60.492855</td>\n",
       "      <td>-56.560219</td>\n",
       "      <td>-55.385315</td>\n",
       "      <td>-56.732849</td>\n",
       "      <td>-56.066483</td>\n",
       "      <td>-55.308147</td>\n",
       "      <td>-56.181320</td>\n",
       "      <td>-57.334415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>-58.788780</td>\n",
       "      <td>-60.382511</td>\n",
       "      <td>-64.728615</td>\n",
       "      <td>-60.276978</td>\n",
       "      <td>-57.266235</td>\n",
       "      <td>-57.788879</td>\n",
       "      <td>-56.029137</td>\n",
       "      <td>-55.534805</td>\n",
       "      <td>-57.560936</td>\n",
       "      <td>-60.581417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>-42.143559</td>\n",
       "      <td>-41.739872</td>\n",
       "      <td>-42.076035</td>\n",
       "      <td>-42.329266</td>\n",
       "      <td>-42.281353</td>\n",
       "      <td>-42.181221</td>\n",
       "      <td>-42.143562</td>\n",
       "      <td>-42.170750</td>\n",
       "      <td>-42.067802</td>\n",
       "      <td>-42.137489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-63.951645</td>\n",
       "      <td>-63.951645</td>\n",
       "      <td>-63.951645</td>\n",
       "      <td>-63.951645</td>\n",
       "      <td>-63.951645</td>\n",
       "      <td>-63.951645</td>\n",
       "      <td>-63.951645</td>\n",
       "      <td>-63.951645</td>\n",
       "      <td>-63.951645</td>\n",
       "      <td>-63.951645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>-56.732780</td>\n",
       "      <td>-56.209183</td>\n",
       "      <td>-57.508713</td>\n",
       "      <td>-58.353237</td>\n",
       "      <td>-60.217651</td>\n",
       "      <td>-58.700066</td>\n",
       "      <td>-58.568951</td>\n",
       "      <td>-57.593288</td>\n",
       "      <td>-56.608555</td>\n",
       "      <td>-57.326683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1080 rows × 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4          5    \\\n",
       "296  -59.765537 -59.535393 -58.336723 -59.388527 -59.765537 -58.550285   \n",
       "1065 -64.228317 -64.228317 -64.228317 -64.228317 -64.228317 -64.228317   \n",
       "1219 -48.397202 -48.397202 -48.397202 -48.397202 -48.397202 -48.397202   \n",
       "1283 -60.439041 -58.920818 -57.311176 -57.387520 -57.312222 -55.221611   \n",
       "286  -55.609592 -56.180172 -56.180172 -55.646084 -55.201511 -56.180172   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1095 -57.752029 -59.759384 -60.492855 -56.560219 -55.385315 -56.732849   \n",
       "1130 -58.788780 -60.382511 -64.728615 -60.276978 -57.266235 -57.788879   \n",
       "1294 -42.143559 -41.739872 -42.076035 -42.329266 -42.281353 -42.181221   \n",
       "860  -63.951645 -63.951645 -63.951645 -63.951645 -63.951645 -63.951645   \n",
       "1126 -56.732780 -56.209183 -57.508713 -58.353237 -60.217651 -58.700066   \n",
       "\n",
       "            6          7          8          9    ...  155  156  157  158  \\\n",
       "296  -59.391659 -59.765537 -59.701347 -58.975872  ...  0.0  0.0  0.0  0.0   \n",
       "1065 -64.228317 -64.228317 -64.228317 -64.282455  ...  0.0  0.0  0.0  0.0   \n",
       "1219 -48.397202 -48.397202 -48.397202 -48.397202  ...  0.0  0.0  0.0  0.0   \n",
       "1283 -54.327877 -56.239277 -58.191216 -59.415623  ...  0.0  0.0  0.0  0.0   \n",
       "286  -56.180172 -56.180172 -56.180172 -56.180172  ...  0.0  0.0  0.0  0.0   \n",
       "...         ...        ...        ...        ...  ...  ...  ...  ...  ...   \n",
       "1095 -56.066483 -55.308147 -56.181320 -57.334415  ...  0.0  0.0  0.0  0.0   \n",
       "1130 -56.029137 -55.534805 -57.560936 -60.581417  ...  0.0  0.0  0.0  0.0   \n",
       "1294 -42.143562 -42.170750 -42.067802 -42.137489  ...  0.0  0.0  0.0  0.0   \n",
       "860  -63.951645 -63.951645 -63.951645 -63.951645  ...  0.0  0.0  0.0  0.0   \n",
       "1126 -58.568951 -57.593288 -56.608555 -57.326683  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      159  160  161  162  163  164  \n",
       "296   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1065  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1219  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1283  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "286   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  \n",
       "1095  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1130  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1294  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "860   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1126  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1080 rows x 165 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data, changed varriable names to avoid conflicts\n",
    "X = df.drop(columns=['Emotions', 'Path'])\n",
    "y = df['Emotions']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, shuffle=True, random_state=42)\n",
    "X_train\n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size : (1080, 165)\n",
      "y_train size : (1080,)\n",
      "X_test size : (360, 165)\n",
      "y_test size : (360,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train size : \" + str(X_train.shape))\n",
    "print(\"y_train size : \" + str(y_train.shape))\n",
    "print(\"X_test size : \" + str(X_test.shape))\n",
    "print(\"y_test size : \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.415611</td>\n",
       "      <td>-0.415175</td>\n",
       "      <td>-0.285009</td>\n",
       "      <td>-0.424523</td>\n",
       "      <td>-0.469390</td>\n",
       "      <td>-0.322287</td>\n",
       "      <td>-0.435815</td>\n",
       "      <td>-0.491443</td>\n",
       "      <td>-0.482199</td>\n",
       "      <td>-0.382752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>-0.944568</td>\n",
       "      <td>-0.977914</td>\n",
       "      <td>-1.011012</td>\n",
       "      <td>-1.026274</td>\n",
       "      <td>-1.022516</td>\n",
       "      <td>-1.026794</td>\n",
       "      <td>-1.036989</td>\n",
       "      <td>-1.045302</td>\n",
       "      <td>-1.041512</td>\n",
       "      <td>-1.037806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>0.931835</td>\n",
       "      <td>0.920429</td>\n",
       "      <td>0.939808</td>\n",
       "      <td>0.942074</td>\n",
       "      <td>0.939622</td>\n",
       "      <td>0.937468</td>\n",
       "      <td>0.930746</td>\n",
       "      <td>0.919440</td>\n",
       "      <td>0.914445</td>\n",
       "      <td>0.923097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>-0.495439</td>\n",
       "      <td>-0.341480</td>\n",
       "      <td>-0.158634</td>\n",
       "      <td>-0.175729</td>\n",
       "      <td>-0.165322</td>\n",
       "      <td>0.090722</td>\n",
       "      <td>0.193590</td>\n",
       "      <td>-0.053812</td>\n",
       "      <td>-0.295620</td>\n",
       "      <td>-0.437036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.076977</td>\n",
       "      <td>-0.012844</td>\n",
       "      <td>-0.019264</td>\n",
       "      <td>0.040791</td>\n",
       "      <td>0.096284</td>\n",
       "      <td>-0.028212</td>\n",
       "      <td>-0.036642</td>\n",
       "      <td>-0.046476</td>\n",
       "      <td>-0.047152</td>\n",
       "      <td>-0.037646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>-0.176958</td>\n",
       "      <td>-0.442035</td>\n",
       "      <td>-0.550702</td>\n",
       "      <td>-0.072868</td>\n",
       "      <td>0.073503</td>\n",
       "      <td>-0.096786</td>\n",
       "      <td>-0.022511</td>\n",
       "      <td>0.061747</td>\n",
       "      <td>-0.047294</td>\n",
       "      <td>-0.180128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>-0.299840</td>\n",
       "      <td>-0.516755</td>\n",
       "      <td>-1.072662</td>\n",
       "      <td>-0.534988</td>\n",
       "      <td>-0.159622</td>\n",
       "      <td>-0.227814</td>\n",
       "      <td>-0.017869</td>\n",
       "      <td>0.033618</td>\n",
       "      <td>-0.217748</td>\n",
       "      <td>-0.580943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>1.673055</td>\n",
       "      <td>1.718724</td>\n",
       "      <td>1.718746</td>\n",
       "      <td>1.696526</td>\n",
       "      <td>1.697632</td>\n",
       "      <td>1.708722</td>\n",
       "      <td>1.708045</td>\n",
       "      <td>1.692182</td>\n",
       "      <td>1.696451</td>\n",
       "      <td>1.695807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-0.911775</td>\n",
       "      <td>-0.944738</td>\n",
       "      <td>-0.976918</td>\n",
       "      <td>-0.991875</td>\n",
       "      <td>-0.988224</td>\n",
       "      <td>-0.992466</td>\n",
       "      <td>-1.002600</td>\n",
       "      <td>-1.010966</td>\n",
       "      <td>-1.007329</td>\n",
       "      <td>-0.996970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>-0.056150</td>\n",
       "      <td>-0.016322</td>\n",
       "      <td>-0.182976</td>\n",
       "      <td>-0.295801</td>\n",
       "      <td>-0.525426</td>\n",
       "      <td>-0.340871</td>\n",
       "      <td>-0.333556</td>\n",
       "      <td>-0.221853</td>\n",
       "      <td>-0.100080</td>\n",
       "      <td>-0.179173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.043017</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.030443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1080 rows × 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "296  -0.415611 -0.415175 -0.285009 -0.424523 -0.469390 -0.322287 -0.435815   \n",
       "1065 -0.944568 -0.977914 -1.011012 -1.026274 -1.022516 -1.026794 -1.036989   \n",
       "1219  0.931835  0.920429  0.939808  0.942074  0.939622  0.937468  0.930746   \n",
       "1283 -0.495439 -0.341480 -0.158634 -0.175729 -0.165322  0.090722  0.193590   \n",
       "286   0.076977 -0.012844 -0.019264  0.040791  0.096284 -0.028212 -0.036642   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1095 -0.176958 -0.442035 -0.550702 -0.072868  0.073503 -0.096786 -0.022511   \n",
       "1130 -0.299840 -0.516755 -1.072662 -0.534988 -0.159622 -0.227814 -0.017869   \n",
       "1294  1.673055  1.718724  1.718746  1.696526  1.697632  1.708722  1.708045   \n",
       "860  -0.911775 -0.944738 -0.976918 -0.991875 -0.988224 -0.992466 -1.002600   \n",
       "1126 -0.056150 -0.016322 -0.182976 -0.295801 -0.525426 -0.340871 -0.333556   \n",
       "\n",
       "           7         8         9    ...       155       156       157  \\\n",
       "296  -0.491443 -0.482199 -0.382752  ...  0.043067  0.043056  0.043036   \n",
       "1065 -1.045302 -1.041512 -1.037806  ...  0.043067  0.043056  0.043036   \n",
       "1219  0.919440  0.914445  0.923097  ...  0.043067  0.043056  0.043036   \n",
       "1283 -0.053812 -0.295620 -0.437036  ...  0.043067  0.043056  0.043036   \n",
       "286  -0.046476 -0.047152 -0.037646  ...  0.043067  0.043056  0.043036   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1095  0.061747 -0.047294 -0.180128  ...  0.043067  0.043056  0.043036   \n",
       "1130  0.033618 -0.217748 -0.580943  ...  0.043067  0.043056  0.043036   \n",
       "1294  1.692182  1.696451  1.695807  ...  0.043067  0.043056  0.043036   \n",
       "860  -1.010966 -1.007329 -0.996970  ...  0.043067  0.043056  0.043036   \n",
       "1126 -0.221853 -0.100080 -0.179173  ...  0.043067  0.043056  0.043036   \n",
       "\n",
       "           158       159       160       161       162       163       164  \n",
       "296   0.043017  0.043027  0.030443  0.030443  0.030443  0.030443  0.030443  \n",
       "1065  0.043017  0.043027  0.030443  0.030443  0.030443  0.030443  0.030443  \n",
       "1219  0.043017  0.043027  0.030443  0.030443  0.030443  0.030443  0.030443  \n",
       "1283  0.043017  0.043027  0.030443  0.030443  0.030443  0.030443  0.030443  \n",
       "286   0.043017  0.043027  0.030443  0.030443  0.030443  0.030443  0.030443  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1095  0.043017  0.043027  0.030443  0.030443  0.030443  0.030443  0.030443  \n",
       "1130  0.043017  0.043027  0.030443  0.030443  0.030443  0.030443  0.030443  \n",
       "1294  0.043017  0.043027  0.030443  0.030443  0.030443  0.030443  0.030443  \n",
       "860   0.043017  0.043027  0.030443  0.030443  0.030443  0.030443  0.030443  \n",
       "1126  0.043017  0.043027  0.030443  0.030443  0.030443  0.030443  0.030443  \n",
       "\n",
       "[1080 rows x 165 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lts do data normalization \n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std\n",
    "\n",
    "# Check the dataset now \n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 165)\n",
      "['angry' 'calm' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# Lets few preparation steps to get it into the correct format for Keras \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# one hot encode the target \n",
    "lb = LabelEncoder()\n",
    "y_train = tf.keras.utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = tf.keras.utils.to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(lb.classes_)\n",
    "#print(y_train[0:10])\n",
    "#print(y_test[0:10])\n",
    "\n",
    "# Pickel the lb object for future use \n",
    "filename = 'labels'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(lb,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 165, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_8 (Conv1D)           (None, 165, 256)          2304      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 165, 256)          0         \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 165, 256)          524544    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 165, 256)         1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 165, 256)          0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 165, 256)          0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 20, 256)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 20, 128)           262272    \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 20, 128)           0         \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 20, 128)           131200    \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 20, 128)           0         \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          (None, 20, 128)           131200    \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 20, 128)           0         \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 20, 128)           131200    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 20, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 20, 128)           0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20, 128)           0         \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 2, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_14 (Conv1D)          (None, 2, 64)             65600     \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 2, 64)             0         \n",
      "                                                                 \n",
      " conv1d_15 (Conv1D)          (None, 2, 64)             32832     \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 2, 64)             0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,283,720\n",
      "Trainable params: 1,282,952\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# New model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(256, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8)) # Target class number\n",
    "model.add(Activation('softmax'))\n",
    "# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# opt = keras.optimizers.Adam(lr=0.0001)\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.00001, decay=1e-6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "68/68 [==============================] - 7s 82ms/step - loss: 2.0438 - accuracy: 0.1769 - val_loss: 2.0787 - val_accuracy: 0.1833\n",
      "Epoch 2/100\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 1.9493 - accuracy: 0.2796 - val_loss: 2.0766 - val_accuracy: 0.1583\n",
      "Epoch 3/100\n",
      "68/68 [==============================] - 5s 74ms/step - loss: 1.9017 - accuracy: 0.2981 - val_loss: 2.0718 - val_accuracy: 0.1667\n",
      "Epoch 4/100\n",
      "68/68 [==============================] - 6s 87ms/step - loss: 1.8698 - accuracy: 0.3213 - val_loss: 2.0633 - val_accuracy: 0.1917\n",
      "Epoch 5/100\n",
      "68/68 [==============================] - 6s 88ms/step - loss: 1.8273 - accuracy: 0.3398 - val_loss: 2.0453 - val_accuracy: 0.2361\n",
      "Epoch 6/100\n",
      "68/68 [==============================] - 6s 89ms/step - loss: 1.8074 - accuracy: 0.3333 - val_loss: 2.0158 - val_accuracy: 0.2583\n",
      "Epoch 7/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.7817 - accuracy: 0.3620 - val_loss: 1.9773 - val_accuracy: 0.2833\n",
      "Epoch 8/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 1.7567 - accuracy: 0.3685 - val_loss: 1.9372 - val_accuracy: 0.2833\n",
      "Epoch 9/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 1.7369 - accuracy: 0.3657 - val_loss: 1.9037 - val_accuracy: 0.2778\n",
      "Epoch 10/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 1.7234 - accuracy: 0.3620 - val_loss: 1.8757 - val_accuracy: 0.2917\n",
      "Epoch 11/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 1.7073 - accuracy: 0.3704 - val_loss: 1.8467 - val_accuracy: 0.3000\n",
      "Epoch 12/100\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 1.6915 - accuracy: 0.3880 - val_loss: 1.8304 - val_accuracy: 0.3222\n",
      "Epoch 13/100\n",
      "68/68 [==============================] - 6s 85ms/step - loss: 1.6691 - accuracy: 0.3954 - val_loss: 1.8182 - val_accuracy: 0.3139\n",
      "Epoch 14/100\n",
      "68/68 [==============================] - 6s 89ms/step - loss: 1.6517 - accuracy: 0.4065 - val_loss: 1.8134 - val_accuracy: 0.3306\n",
      "Epoch 15/100\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 1.6321 - accuracy: 0.4213 - val_loss: 1.7997 - val_accuracy: 0.3222\n",
      "Epoch 16/100\n",
      "68/68 [==============================] - 6s 87ms/step - loss: 1.6253 - accuracy: 0.4139 - val_loss: 1.7882 - val_accuracy: 0.3194\n",
      "Epoch 17/100\n",
      "68/68 [==============================] - 6s 91ms/step - loss: 1.6088 - accuracy: 0.4250 - val_loss: 1.7827 - val_accuracy: 0.3306\n",
      "Epoch 18/100\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 1.5791 - accuracy: 0.4343 - val_loss: 1.7678 - val_accuracy: 0.3389\n",
      "Epoch 19/100\n",
      "68/68 [==============================] - 6s 84ms/step - loss: 1.5670 - accuracy: 0.4352 - val_loss: 1.7666 - val_accuracy: 0.3444\n",
      "Epoch 20/100\n",
      "68/68 [==============================] - 6s 85ms/step - loss: 1.5591 - accuracy: 0.4380 - val_loss: 1.7595 - val_accuracy: 0.3472\n",
      "Epoch 21/100\n",
      "68/68 [==============================] - 6s 86ms/step - loss: 1.5409 - accuracy: 0.4593 - val_loss: 1.7468 - val_accuracy: 0.3472\n",
      "Epoch 22/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 1.5297 - accuracy: 0.4685 - val_loss: 1.7537 - val_accuracy: 0.3583\n",
      "Epoch 23/100\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 1.5194 - accuracy: 0.4667 - val_loss: 1.7306 - val_accuracy: 0.3556\n",
      "Epoch 24/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.5058 - accuracy: 0.4639 - val_loss: 1.7220 - val_accuracy: 0.3528\n",
      "Epoch 25/100\n",
      "68/68 [==============================] - 6s 86ms/step - loss: 1.4750 - accuracy: 0.4954 - val_loss: 1.7168 - val_accuracy: 0.3667\n",
      "Epoch 26/100\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 1.4708 - accuracy: 0.4880 - val_loss: 1.7073 - val_accuracy: 0.3722\n",
      "Epoch 27/100\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 1.4420 - accuracy: 0.5009 - val_loss: 1.6980 - val_accuracy: 0.3750\n",
      "Epoch 28/100\n",
      "68/68 [==============================] - 5s 78ms/step - loss: 1.4314 - accuracy: 0.5185 - val_loss: 1.6867 - val_accuracy: 0.3722\n",
      "Epoch 29/100\n",
      "68/68 [==============================] - 5s 81ms/step - loss: 1.4122 - accuracy: 0.5259 - val_loss: 1.6767 - val_accuracy: 0.3722\n",
      "Epoch 30/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.4100 - accuracy: 0.5102 - val_loss: 1.6792 - val_accuracy: 0.3917\n",
      "Epoch 31/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.3788 - accuracy: 0.5463 - val_loss: 1.6643 - val_accuracy: 0.3889\n",
      "Epoch 32/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.3741 - accuracy: 0.5278 - val_loss: 1.6570 - val_accuracy: 0.3917\n",
      "Epoch 33/100\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 1.3562 - accuracy: 0.5426 - val_loss: 1.6488 - val_accuracy: 0.4111\n",
      "Epoch 34/100\n",
      "68/68 [==============================] - 6s 93ms/step - loss: 1.3422 - accuracy: 0.5583 - val_loss: 1.6524 - val_accuracy: 0.4028\n",
      "Epoch 35/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 1.3232 - accuracy: 0.5546 - val_loss: 1.6472 - val_accuracy: 0.4028\n",
      "Epoch 36/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.3280 - accuracy: 0.5657 - val_loss: 1.6281 - val_accuracy: 0.4139\n",
      "Epoch 37/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 1.2976 - accuracy: 0.5750 - val_loss: 1.6288 - val_accuracy: 0.4278\n",
      "Epoch 38/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.2824 - accuracy: 0.5778 - val_loss: 1.6494 - val_accuracy: 0.4139\n",
      "Epoch 39/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.2615 - accuracy: 0.5944 - val_loss: 1.6096 - val_accuracy: 0.4139\n",
      "Epoch 40/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 1.2430 - accuracy: 0.5981 - val_loss: 1.6040 - val_accuracy: 0.4222\n",
      "Epoch 41/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 1.2435 - accuracy: 0.6120 - val_loss: 1.6038 - val_accuracy: 0.4083\n",
      "Epoch 42/100\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 1.2224 - accuracy: 0.5963 - val_loss: 1.5904 - val_accuracy: 0.4361\n",
      "Epoch 43/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 1.2146 - accuracy: 0.6074 - val_loss: 1.5889 - val_accuracy: 0.4167\n",
      "Epoch 44/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.2015 - accuracy: 0.6009 - val_loss: 1.5811 - val_accuracy: 0.4528\n",
      "Epoch 45/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 1.1863 - accuracy: 0.6139 - val_loss: 1.5715 - val_accuracy: 0.4361\n",
      "Epoch 46/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.1706 - accuracy: 0.6065 - val_loss: 1.5939 - val_accuracy: 0.4361\n",
      "Epoch 47/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.1509 - accuracy: 0.6324 - val_loss: 1.5696 - val_accuracy: 0.4667\n",
      "Epoch 48/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.1419 - accuracy: 0.6343 - val_loss: 1.5740 - val_accuracy: 0.4389\n",
      "Epoch 49/100\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 1.1362 - accuracy: 0.6380 - val_loss: 1.5620 - val_accuracy: 0.4556\n",
      "Epoch 50/100\n",
      "68/68 [==============================] - 6s 87ms/step - loss: 1.1032 - accuracy: 0.6463 - val_loss: 1.5717 - val_accuracy: 0.4278\n",
      "Epoch 51/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.1036 - accuracy: 0.6667 - val_loss: 1.5478 - val_accuracy: 0.4361\n",
      "Epoch 52/100\n",
      "68/68 [==============================] - 5s 81ms/step - loss: 1.0769 - accuracy: 0.6583 - val_loss: 1.5427 - val_accuracy: 0.4500\n",
      "Epoch 53/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.0753 - accuracy: 0.6694 - val_loss: 1.5372 - val_accuracy: 0.4528\n",
      "Epoch 54/100\n",
      "68/68 [==============================] - 6s 84ms/step - loss: 1.0443 - accuracy: 0.6843 - val_loss: 1.5414 - val_accuracy: 0.4667\n",
      "Epoch 55/100\n",
      "68/68 [==============================] - 6s 85ms/step - loss: 1.0357 - accuracy: 0.6870 - val_loss: 1.5379 - val_accuracy: 0.4611\n",
      "Epoch 56/100\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 1.0227 - accuracy: 0.6824 - val_loss: 1.5307 - val_accuracy: 0.4722\n",
      "Epoch 57/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 1.0029 - accuracy: 0.7019 - val_loss: 1.5275 - val_accuracy: 0.4444\n",
      "Epoch 58/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 0.9993 - accuracy: 0.7019 - val_loss: 1.5204 - val_accuracy: 0.4500\n",
      "Epoch 59/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 0.9842 - accuracy: 0.7065 - val_loss: 1.5191 - val_accuracy: 0.4472\n",
      "Epoch 60/100\n",
      "68/68 [==============================] - 5s 81ms/step - loss: 0.9812 - accuracy: 0.6889 - val_loss: 1.5138 - val_accuracy: 0.4583\n",
      "Epoch 61/100\n",
      "68/68 [==============================] - 5s 78ms/step - loss: 0.9533 - accuracy: 0.7167 - val_loss: 1.5019 - val_accuracy: 0.4778\n",
      "Epoch 62/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 0.9484 - accuracy: 0.7259 - val_loss: 1.5136 - val_accuracy: 0.4500\n",
      "Epoch 63/100\n",
      "68/68 [==============================] - 5s 78ms/step - loss: 0.9292 - accuracy: 0.7315 - val_loss: 1.5016 - val_accuracy: 0.4667\n",
      "Epoch 64/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 0.9369 - accuracy: 0.7269 - val_loss: 1.5233 - val_accuracy: 0.4222\n",
      "Epoch 65/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 0.9053 - accuracy: 0.7333 - val_loss: 1.5045 - val_accuracy: 0.4639\n",
      "Epoch 66/100\n",
      "68/68 [==============================] - 5s 81ms/step - loss: 0.8963 - accuracy: 0.7481 - val_loss: 1.4984 - val_accuracy: 0.4722\n",
      "Epoch 67/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 0.8812 - accuracy: 0.7463 - val_loss: 1.4933 - val_accuracy: 0.4472\n",
      "Epoch 68/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 0.8572 - accuracy: 0.7537 - val_loss: 1.5024 - val_accuracy: 0.4306\n",
      "Epoch 69/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 0.8508 - accuracy: 0.7620 - val_loss: 1.4964 - val_accuracy: 0.4472\n",
      "Epoch 70/100\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 0.8505 - accuracy: 0.7565 - val_loss: 1.4869 - val_accuracy: 0.4500\n",
      "Epoch 71/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 0.8252 - accuracy: 0.7769 - val_loss: 1.4819 - val_accuracy: 0.4472\n",
      "Epoch 72/100\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 0.8033 - accuracy: 0.7889 - val_loss: 1.4811 - val_accuracy: 0.4417\n",
      "Epoch 73/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 0.7990 - accuracy: 0.7889 - val_loss: 1.4745 - val_accuracy: 0.4611\n",
      "Epoch 74/100\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 0.7771 - accuracy: 0.8028 - val_loss: 1.4650 - val_accuracy: 0.4583\n",
      "Epoch 75/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 0.7710 - accuracy: 0.8056 - val_loss: 1.4798 - val_accuracy: 0.4528\n",
      "Epoch 76/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 0.7635 - accuracy: 0.7889 - val_loss: 1.4758 - val_accuracy: 0.4417\n",
      "Epoch 77/100\n",
      "68/68 [==============================] - 6s 84ms/step - loss: 0.7429 - accuracy: 0.8176 - val_loss: 1.4608 - val_accuracy: 0.4778\n",
      "Epoch 78/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 0.7319 - accuracy: 0.8074 - val_loss: 1.4640 - val_accuracy: 0.4528\n",
      "Epoch 79/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 0.7086 - accuracy: 0.8139 - val_loss: 1.4663 - val_accuracy: 0.4500\n",
      "Epoch 80/100\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 0.7119 - accuracy: 0.8065 - val_loss: 1.4663 - val_accuracy: 0.4722\n",
      "Epoch 81/100\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 0.7022 - accuracy: 0.8176 - val_loss: 1.4437 - val_accuracy: 0.4778\n",
      "Epoch 82/100\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 0.6827 - accuracy: 0.8287 - val_loss: 1.4370 - val_accuracy: 0.4861\n",
      "Epoch 83/100\n",
      "68/68 [==============================] - 6s 84ms/step - loss: 0.6820 - accuracy: 0.8269 - val_loss: 1.4568 - val_accuracy: 0.4722\n",
      "Epoch 84/100\n",
      "68/68 [==============================] - 6s 84ms/step - loss: 0.6687 - accuracy: 0.8278 - val_loss: 1.4411 - val_accuracy: 0.4694\n",
      "Epoch 85/100\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 0.6626 - accuracy: 0.8417 - val_loss: 1.4411 - val_accuracy: 0.4722\n",
      "Epoch 86/100\n",
      "68/68 [==============================] - 5s 79ms/step - loss: 0.6312 - accuracy: 0.8602 - val_loss: 1.4517 - val_accuracy: 0.4556\n",
      "Epoch 87/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 0.6199 - accuracy: 0.8611 - val_loss: 1.4444 - val_accuracy: 0.4667\n",
      "Epoch 88/100\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 0.6164 - accuracy: 0.8556 - val_loss: 1.4425 - val_accuracy: 0.4583\n",
      "Epoch 89/100\n",
      "68/68 [==============================] - 6s 82ms/step - loss: 0.6141 - accuracy: 0.8481 - val_loss: 1.4404 - val_accuracy: 0.4667\n",
      "Epoch 90/100\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 0.5933 - accuracy: 0.8537 - val_loss: 1.4604 - val_accuracy: 0.4417\n",
      "Epoch 91/100\n",
      "68/68 [==============================] - 6s 86ms/step - loss: 0.5803 - accuracy: 0.8741 - val_loss: 1.4544 - val_accuracy: 0.4278\n",
      "Epoch 92/100\n",
      "68/68 [==============================] - 6s 81ms/step - loss: 0.5623 - accuracy: 0.8833 - val_loss: 1.4479 - val_accuracy: 0.4361\n",
      "Epoch 93/100\n",
      "68/68 [==============================] - 6s 84ms/step - loss: 0.5482 - accuracy: 0.8778 - val_loss: 1.4319 - val_accuracy: 0.4833\n",
      "Epoch 94/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 0.5444 - accuracy: 0.8667 - val_loss: 1.4299 - val_accuracy: 0.4611\n",
      "Epoch 95/100\n",
      "68/68 [==============================] - 6s 83ms/step - loss: 0.5466 - accuracy: 0.8778 - val_loss: 1.4308 - val_accuracy: 0.4667\n",
      "Epoch 96/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 0.5146 - accuracy: 0.9019 - val_loss: 1.4272 - val_accuracy: 0.4389\n",
      "Epoch 97/100\n",
      "68/68 [==============================] - 5s 80ms/step - loss: 0.5159 - accuracy: 0.8852 - val_loss: 1.4337 - val_accuracy: 0.4861\n",
      "Epoch 98/100\n",
      "68/68 [==============================] - 6s 89ms/step - loss: 0.4981 - accuracy: 0.8981 - val_loss: 1.4030 - val_accuracy: 0.4833\n",
      "Epoch 99/100\n",
      "68/68 [==============================] - 7s 107ms/step - loss: 0.5057 - accuracy: 0.8852 - val_loss: 1.4160 - val_accuracy: 0.4833\n",
      "Epoch 100/100\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.4784 - accuracy: 0.8944 - val_loss: 1.4076 - val_accuracy: 0.4694\n"
     ]
    }
   ],
   "source": [
    "#adam = optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "model_history=model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3358073929bd3d27ff594bc6528257efc4213b34ba9d7c6bd240dce3a23a83d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('torch_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
