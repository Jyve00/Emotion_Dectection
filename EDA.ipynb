{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%matplotlib_inline` not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os \n",
    "import sys \n",
    "import torch \n",
    "import torchaudio \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib_inline\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploritory Analysis \n",
    "\n",
    "The RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) is a widely used dataset for emotion classification using recorded speech because of its high quality and consistent audio quality. The dataset can be found at https://smartlaboratory.org/ravdess/ and more info can be found in this offical citation from the creators: \n",
    "\n",
    "Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
    "\n",
    "RAVDESS contains both audio and video but for this project i will only be using and discussions the audio-only portion of the dataset. The database contains audio from 24 actors (12 male, 12 female) each speaking 2 similar sentences in a neutral North American accent. Each statement is spoken in 8 different emotions/ expressions (calm, happy, sad, angry, fearful, suprise, and disgust). Each one is performed in 2 different levels of emotional intensity (normal, strong) and a neutral expression is added. All audio recordings have a sample rate of 48kHz with a bit depth of 16bit. There is a total of 1440 audio files (24 actors X 60 trials per actor).\n",
    "\n",
    "\n",
    "RAVDESS does not come with any sort of metadata table with information on the recordings but instead the filename themselves have all the information. Each filename has a 7 part numerical identifier (ex. 03-01-04-01-01-02-12.wav). The identifiers represent the following: \n",
    "\n",
    "    1.) Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "    2.) Vocal channel (01 = speech, 02 = song).\n",
    "    3.) Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "    4.) Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
    "    5.) Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
    "    6.) Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "    7.) Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "So for example the file 03-01-04-01-01-02-12.wav contains the following metadata: \n",
    "\n",
    "    1.) Audio-only (03)\n",
    "    2.) Speech (01)\n",
    "    3.) Sad (04)\n",
    "    4.) Normal Intensity (01)\n",
    "    5.) \"Kids are talking by the door\" (01)\n",
    "    6.) Second Repetition (02)\n",
    "    7.) Actor-12 Male (12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Metadata table\n",
    "\n",
    "To make the audio data easier to deal with I will create a Pandas data frame that will contain the file path of each audio file and linked to it's emotion as it will be are target variable. The audio files are each separated into their own folders by which actor performed them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Statement</th>\n",
       "      <th>actor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>intense</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>normal</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "      <td>intense</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotions                                               Path Intensity  \\\n",
       "0  surprise  /Users/stephen/Emotion_Dectection/data/RAVDESS...   intense   \n",
       "1  surprise  /Users/stephen/Emotion_Dectection/data/RAVDESS...    normal   \n",
       "2     angry  /Users/stephen/Emotion_Dectection/data/RAVDESS...    normal   \n",
       "3      fear  /Users/stephen/Emotion_Dectection/data/RAVDESS...    normal   \n",
       "4      fear  /Users/stephen/Emotion_Dectection/data/RAVDESS...   intense   \n",
       "\n",
       "                      Statement  actor  \n",
       "0  Dogs are sitting by the door      1  \n",
       "1  Kids are talking by the door      1  \n",
       "2  Dogs are sitting by the door      1  \n",
       "3  Dogs are sitting by the door      1  \n",
       "4  Kids are talking by the door      1  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path = '/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/'\n",
    "\n",
    "dir_list = os.listdir(audio_path)\n",
    "dir_list.sort()\n",
    "\n",
    "emotion = []\n",
    "path = []\n",
    "intensity = []\n",
    "statement = []\n",
    "actor = []\n",
    "gender = []\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(audio_path + i)\n",
    "    for f in fname:\n",
    "        part = f.split('.')[0].split('-')\n",
    "        emotion.append(int(part[2]))\n",
    "        temp = int(part[6])\n",
    "        if temp%2 == 0:\n",
    "            temp = \"female\"\n",
    "        else:\n",
    "            temp = \"male\"\n",
    "        gender.append(temp)\n",
    "        path.append(audio_path + i + '/' + f)\n",
    "\n",
    "        intent = int(part[3])\n",
    "        if intent == 1:\n",
    "            intent = \"normal\"\n",
    "        else: \n",
    "            intent = \"intense\"\n",
    "        intensity.append(intent)\n",
    "        state = int(part[4])\n",
    "        if state == 1:\n",
    "            state = \"Kids are talking by the door\"\n",
    "        else: \n",
    "            state = \"Dogs are sitting by the door\"\n",
    "        statement.append(state)\n",
    "        act = int(part[6])\n",
    "        actor.append(act)\n",
    "\n",
    "\n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for emotional intensity \n",
    "intensity_df = pd.DataFrame(intensity, columns=['Intensity'])\n",
    "\n",
    "# dateframe for statements\n",
    "statement_df = pd.DataFrame(statement, columns=['Statement'])\n",
    "\n",
    "# dataframe for actor number \n",
    "actor_df = pd.DataFrame(actor, columns=['actor'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df, intensity_df, statement_df, actor_df], axis=1)\n",
    "\n",
    "# changing integers to actual emotions.\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "Ravdess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "surprise    192\n",
       "angry       192\n",
       "fear        192\n",
       "disgust     192\n",
       "sad         192\n",
       "happy       192\n",
       "calm        192\n",
       "neutral      96\n",
       "Name: Emotions, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take a look at our target variables \n",
    "Ravdess_df['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset is all balanced except for the \"neutral\" emotion. This doesn't seem like it will be a problem so we'll leave it as is for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3358073929bd3d27ff594bc6528257efc4213b34ba9d7c6bd240dce3a23a83d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('torch_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
