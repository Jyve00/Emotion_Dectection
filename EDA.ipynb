{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os \n",
    "import sys \n",
    "import torch \n",
    "import torchaudio \n",
    "import matplotlib.pyplot as plt \n",
    "#%matplotlib_inline\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torchaudio_functions as Audio\n",
    "from torchaudio_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploritory Analysis \n",
    "\n",
    "The RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) is a widely used dataset for emotion classification using recorded speech because of its high quality and consistent audio quality. The dataset can be found at https://smartlaboratory.org/ravdess/ and more info can be found in this offical citation from the creators: \n",
    "\n",
    "Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
    "\n",
    "RAVDESS contains both audio and video but for this project i will only be using and discussions the audio-only portion of the dataset. The database contains audio from 24 actors (12 male, 12 female) each speaking 2 similar sentences in a neutral North American accent. Each statement is spoken in 8 different emotions/ expressions (calm, happy, sad, angry, fearful, suprise, and disgust). Each one is performed in 2 different levels of emotional intensity (normal, strong) and a neutral expression is added. All audio recordings have a sample rate of 48kHz with a bit depth of 16bit. There is a total of 1440 audio files (24 actors X 60 trials per actor).\n",
    "\n",
    "\n",
    "RAVDESS does not come with any sort of metadata table with information on the recordings but instead the filename themselves have all the information. Each filename has a 7 part numerical identifier (ex. 03-01-04-01-01-02-12.wav). The identifiers represent the following: \n",
    "\n",
    "    1.) Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "    2.) Vocal channel (01 = speech, 02 = song).\n",
    "    3.) Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "    4.) Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
    "    5.) Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
    "    6.) Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "    7.) Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "So for example the file 03-01-04-01-01-02-12.wav contains the following metadata: \n",
    "\n",
    "    1.) Audio-only (03)\n",
    "    2.) Speech (01)\n",
    "    3.) Sad (04)\n",
    "    4.) Normal Intensity (01)\n",
    "    5.) \"Kids are talking by the door\" (01)\n",
    "    6.) Second Repetition (02)\n",
    "    7.) Actor-12 Male (12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Metadata table\n",
    "\n",
    "To make the audio data easier to deal with I will create a Pandas data frame that will contain the file path of each audio file and linked to it's emotion as it will be are target variable. The audio files are each separated into their own folders by which actor performed them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Actor_01',\n",
       " 'Actor_02',\n",
       " 'Actor_03',\n",
       " 'Actor_04',\n",
       " 'Actor_05',\n",
       " 'Actor_06',\n",
       " 'Actor_07',\n",
       " 'Actor_08',\n",
       " 'Actor_09',\n",
       " 'Actor_10',\n",
       " 'Actor_11',\n",
       " 'Actor_12',\n",
       " 'Actor_13',\n",
       " 'Actor_14',\n",
       " 'Actor_15',\n",
       " 'Actor_16',\n",
       " 'Actor_17',\n",
       " 'Actor_18',\n",
       " 'Actor_19',\n",
       " 'Actor_20',\n",
       " 'Actor_21',\n",
       " 'Actor_22',\n",
       " 'Actor_23']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dictionaries with Key Value pairs decoding file path \n",
    "\n",
    "modality = {'01':'full_av','02':'video_only','03':'audio_only'}\n",
    "vocal_channel = {'01':'speech','02':'song'}\n",
    "emotion = {'01':'neutral','02':'calm','03':'happy','04':'sad','05':'angry','06':'fearful','07':'disgust','08':'surprised'}\n",
    "emotional_intensity = {'01':'normal','02':'strong'}\n",
    "statement = {'01':'Kids are talking by the door','02':'Dogs are sitting by the door'}\n",
    "reptition = {'01':'first_repitition','02':'second_repetition'}\n",
    "def actor_f(num):\n",
    "    if int(num)%2==0: return('female')\n",
    "    else: return('male')\n",
    "\n",
    "actors = sorted(os.listdir('/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24'))\n",
    "actors.pop()\n",
    "actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modality</th>\n",
       "      <th>vocal_channel</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotional_intensity</th>\n",
       "      <th>statement</th>\n",
       "      <th>repetition</th>\n",
       "      <th>actor</th>\n",
       "      <th>actor_sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-08-02-02-01-01.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>surprised</td>\n",
       "      <td>strong</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>first_repitition</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-08-01-01-01-01.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>surprised</td>\n",
       "      <td>normal</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>first_repitition</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-05-01-02-01-01.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>angry</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>first_repitition</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-06-01-02-02-01.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>fearful</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>second_repetition</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_01/03-01-06-02-01-02-01.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>fearful</td>\n",
       "      <td>strong</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>second_repetition</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-03-02-02-02-23.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>happy</td>\n",
       "      <td>strong</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>second_repetition</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-03-01-01-02-23.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>happy</td>\n",
       "      <td>normal</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>second_repetition</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-02-02-01-01-23.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>calm</td>\n",
       "      <td>strong</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>first_repitition</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-02-01-02-01-23.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>calm</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>first_repitition</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_23/03-01-01-01-02-02-23.wav</th>\n",
       "      <td>audio_only</td>\n",
       "      <td>speech</td>\n",
       "      <td>neutral</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>second_repetition</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1380 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      modality vocal_channel  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "...                                                        ...           ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  audio_only        speech   \n",
       "\n",
       "                                                      emotion  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  surprised   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  surprised   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      angry   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...    fearful   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...    fearful   \n",
       "...                                                       ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      happy   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      happy   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       calm   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...       calm   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...    neutral   \n",
       "\n",
       "                                                   emotional_intensity  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              strong   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              normal   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              normal   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              normal   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              strong   \n",
       "...                                                                ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              strong   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              normal   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              strong   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              normal   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...              normal   \n",
       "\n",
       "                                                                       statement  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Dogs are sitting by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Kids are talking by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Dogs are sitting by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Dogs are sitting by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Kids are talking by the door   \n",
       "...                                                                          ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Dogs are sitting by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Kids are talking by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Kids are talking by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Dogs are sitting by the door   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  Dogs are sitting by the door   \n",
       "\n",
       "                                                           repetition actor  \\\n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...   first_repitition    01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...   first_repitition    01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...   first_repitition    01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  second_repetition    01   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  second_repetition    01   \n",
       "...                                                               ...   ...   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  second_repetition    23   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  second_repetition    23   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...   first_repitition    23   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...   first_repitition    23   \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...  second_repetition    23   \n",
       "\n",
       "                                                   actor_sex  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "...                                                      ...  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "/Users/stephen/Emotion_Dectection/data/RAVDESS/...      male  \n",
       "\n",
       "[1380 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file_dict = {}\n",
    "for actor in actors:\n",
    "    actor_dir = os.path.join('/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24',actor)\n",
    "    actor_files = os.listdir(actor_dir)\n",
    "    actor_dict = [i.replace(\".wav\",\"\").split(\"-\") for i in actor_files]\n",
    "    dict_entry = {os.path.join(actor_dir,i):j for i,j in zip(actor_files,actor_dict)}\n",
    "    audio_file_dict.update(dict_entry)\n",
    "\n",
    "audio_file_dict = pd.DataFrame(audio_file_dict).T\n",
    "audio_file_dict.columns = ['modality','vocal_channel','emotion','emotional_intensity','statement','repetition','actor']\n",
    "\n",
    "\n",
    "audio_file_dict.modality = audio_file_dict.modality.map(modality)\n",
    "audio_file_dict.vocal_channel = audio_file_dict.vocal_channel.map(vocal_channel)\n",
    "audio_file_dict.emotion = audio_file_dict.emotion.map(emotion)\n",
    "audio_file_dict.emotional_intensity = audio_file_dict.emotional_intensity.map(emotional_intensity)\n",
    "audio_file_dict.statement = audio_file_dict.statement.map(statement)\n",
    "audio_file_dict.repetition = audio_file_dict.repetition.map(reptition)\n",
    "audio_file_dict['actor_sex'] = audio_file_dict.actor.apply(actor_f)\n",
    "\n",
    "audio_file_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe for easy use in the future, export with Pandas.to_csv()\n",
    "\n",
    "audio_file_dict.to_csv('/Users/stephen/Emotion_Dectection/data/RAVDESS/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "surprised    184\n",
       "angry        184\n",
       "fearful      184\n",
       "disgust      184\n",
       "sad          184\n",
       "happy        184\n",
       "calm         184\n",
       "neutral       92\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take a look at our target variables \n",
    "audio_file_dict['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset is all balanced except for the \"neutral\" emotion. This doesn't seem like it will be a problem so we'll leave it as is for now. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning the dataset into a Pytorch Dataset Object \n",
    "We can use Pytorch to turn our collection of audio files into a Pytorch object so it'll be easier to reuse. \n",
    "Since the intention is to input the data into a Neural Network, every observation but be the same length  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140941, 253053)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make all audio files the same shape \n",
    "# make a list with all the audio files to easily see the shapes of each file\n",
    "audio_files = []\n",
    "for i in list(audio_file_dict.index):\n",
    "    i, sr = torchaudio.load(i)\n",
    "    audio_files.append(i)\n",
    "\n",
    "# Iterate through the data and find the maximum and minimum length \n",
    "maxlen = 0\n",
    "minlen = np.Inf\n",
    "for i in audio_files:\n",
    "    if i.shape[1]>maxlen:\n",
    "        maxlen = i.shape[1]\n",
    "    if i.shape[1]<minlen:\n",
    "        minlen = i.shape[1]\n",
    "\n",
    "minlen, maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make a list of mel-spectrograms to get the sizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495, 64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_spectrograms = []\n",
    "for i in audio_files:\n",
    "    mel_spect = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=1024, hop_length=512, n_mels=64)(i)\n",
    "    mel_spectrograms.append(mel_spect)\n",
    "\n",
    "\n",
    "max_width, max_height = max([i.shape[2] for i in mel_spectrograms]), max([i.shape[1] for i in mel_spectrograms])\n",
    "\n",
    "max_width, max_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_file_dict):\n",
    "        self.audio_file_dict = audio_file_dict.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        count = len(self.audio_file_dict)\n",
    "        return count\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            path = self.audio_file_dict.reset_index()['index'][index]\n",
    "            signal, sr = torchaudio.load(path)\n",
    "            signal = torch.mean(signal, dim=0).unsqueeze(0)\n",
    "            img = torchaudio.transforms.Spectrogram()(img)\n",
    "            img = F.pad(img, [0, max_width - img.size(2), 0, max_height - img.size(1)])\n",
    "            label = list(pd.get_dummies(self.audio_file_dict.reset_index().emotion).iloc[index].values)\n",
    "            label = np.array(label)\n",
    "            label = torch.from_numpy(label)\n",
    "            return (img, label)\n",
    "\n",
    "if __\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the audio data\n",
    "Here we'll use the Pytorch module torchaudio to convert the .wav files to tensor object so we can visualize the signal and then perform the necessary feature extractions to obtain data to train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare a variable that will locate a wav file to extract from \n",
    "angry_file_row = Ravdess_df.loc[Ravdess_df['Emotions'] == 'angry']\n",
    "test_file_row = angry_file_row.iloc[0]\n",
    "test_file_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on Pytorch Documents \n",
    "# https://pytorch.org/tutorials/beginner/audio_feature_extractions_tutorial.html\n",
    "\n",
    "\n",
    "\n",
    "# global variables \n",
    "n_fft = 2048\n",
    "n_mels = 128\n",
    "n_mfcc = 128\n",
    "win_length = None \n",
    "hop_length = 512\n",
    "sample_rate = None\n",
    "\n",
    "\n",
    "# load audio file path to tenso \n",
    "waveform, sample_rate = torchaudio.load(test_file_row['Path'])\n",
    "\n",
    "print_stats(waveform, sample_rate=sample_rate)\n",
    "plot_waveform(waveform, sample_rate=sample_rate)\n",
    "plot_specgram(waveform, sample_rate)\n",
    "play_audio(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = T.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    ")\n",
    "# Perform transformation\n",
    "spec = spectrogram(waveform)\n",
    "\n",
    "print_stats(spec)\n",
    "plot_spectrogram(spec[0], title='torchaudio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = T.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    onesided=True,\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    ")\n",
    "\n",
    "melspec = mel_spectrogram(waveform)\n",
    "plot_spectrogram(\n",
    "    melspec[0], title=\"MelSpectrogram - torchaudio\", ylabel='mel freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_transform = torchaudio.transforms.MFCC(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mfcc=n_mfcc,\n",
    "    melkwargs={\n",
    "      'n_fft': n_fft,\n",
    "      'n_mels': n_mels,\n",
    "      'hop_length': hop_length,\n",
    "      'mel_scale': 'htk',\n",
    "    })\n",
    "\n",
    "mfcc = mfcc_transform(waveform)\n",
    "\n",
    "plot_spectrogram(mfcc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch = torchaudio.functional.detect_pitch_frequency(waveform, sample_rate)\n",
    "plot_pitch(waveform, sample_rate, pitch)\n",
    "play_audio(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_feature = torchaudio.functional.compute_kaldi_pitch(waveform, sample_rate)\n",
    "pitch, nfcc = pitch_feature[..., 0], pitch_feature[..., 1]\n",
    "\n",
    "plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc)\n",
    "play_audio(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Feature Augmentation \n",
    "https://pytorch.org/tutorials/beginner/audio_feature_augmentation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a Pytorch dataframe odject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# https://www.youtube.com/watch?v=88FFnqt5MNI&ab_channel=ValerioVelardo-TheSoundofAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating CSV with labels and path name \n",
    "# https://www.kaggle.com/shivamburnwal/speech-emotion-recognition\n",
    "\n",
    "\n",
    "Ravdess = '/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/'\n",
    "\n",
    "ravdess_directory_list = os.listdir(Ravdess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for dir in ravdess_directory_list:\n",
    "    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
    "    actor = os.listdir(Ravdess + dir)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('-')\n",
    "        # third part in each file represents the emotion associated to that file.\n",
    "        file_emotion.append(int(part[2]))\n",
    "        file_path.append(Ravdess + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# changing integers to actual emotions.\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "Ravdess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ravdess_df['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3358073929bd3d27ff594bc6528257efc4213b34ba9d7c6bd240dce3a23a83d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('torch_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
