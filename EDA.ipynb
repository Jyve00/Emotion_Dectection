{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os \n",
    "import sys \n",
    "import torch \n",
    "import torchaudio \n",
    "import matplotlib.pyplot as plt \n",
    "#%matplotlib_inline\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torchaudio_functions as Audio\n",
    "from torchaudio_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploritory Analysis \n",
    "\n",
    "The RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) is a widely used dataset for emotion classification using recorded speech because of its high quality and consistent audio quality. The dataset can be found at https://smartlaboratory.org/ravdess/ and more info can be found in this offical citation from the creators: \n",
    "\n",
    "Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
    "\n",
    "RAVDESS contains both audio and video but for this project i will only be using and discussions the audio-only portion of the dataset. The database contains audio from 24 actors (12 male, 12 female) each speaking 2 similar sentences in a neutral North American accent. Each statement is spoken in 8 different emotions/ expressions (calm, happy, sad, angry, fearful, suprise, and disgust). Each one is performed in 2 different levels of emotional intensity (normal, strong) and a neutral expression is added. All audio recordings have a sample rate of 48kHz with a bit depth of 16bit. There is a total of 1440 audio files (24 actors X 60 trials per actor).\n",
    "\n",
    "\n",
    "RAVDESS does not come with any sort of metadata table with information on the recordings but instead the filename themselves have all the information. Each filename has a 7 part numerical identifier (ex. 03-01-04-01-01-02-12.wav). The identifiers represent the following: \n",
    "\n",
    "    1.) Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "    2.) Vocal channel (01 = speech, 02 = song).\n",
    "    3.) Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "    4.) Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
    "    5.) Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
    "    6.) Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "    7.) Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "So for example the file 03-01-04-01-01-02-12.wav contains the following metadata: \n",
    "\n",
    "    1.) Audio-only (03)\n",
    "    2.) Speech (01)\n",
    "    3.) Sad (04)\n",
    "    4.) Normal Intensity (01)\n",
    "    5.) \"Kids are talking by the door\" (01)\n",
    "    6.) Second Repetition (02)\n",
    "    7.) Actor-12 Male (12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Metadata table\n",
    "\n",
    "To make the audio data easier to deal with I will create a Pandas data frame that will contain the file path of each audio file and linked to it's emotion as it will be are target variable. The audio files are each separated into their own folders by which actor performed them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                               Path\n",
       "0    angry  /Users/stephen/Emotion_Dectection/data/RAVDESS...\n",
       "1     fear  /Users/stephen/Emotion_Dectection/data/RAVDESS...\n",
       "2     fear  /Users/stephen/Emotion_Dectection/data/RAVDESS...\n",
       "3    angry  /Users/stephen/Emotion_Dectection/data/RAVDESS...\n",
       "4  disgust  /Users/stephen/Emotion_Dectection/data/RAVDESS..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path = '/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/'\n",
    "\n",
    "dir_list = os.listdir(audio_path)\n",
    "dir_list.sort()\n",
    "\n",
    "\n",
    "# creating CSV with labels and path name \n",
    "# https://www.kaggle.com/shivamburnwal/speech-emotion-recognition\n",
    "\n",
    "\n",
    "Ravdess = '/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/'\n",
    "\n",
    "ravdess_directory_list = os.listdir(Ravdess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for dir in ravdess_directory_list:\n",
    "    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
    "    actor = os.listdir(Ravdess + dir)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('-')\n",
    "        # third part in each file represents the emotion associated to that file.\n",
    "        file_emotion.append(int(part[2]))\n",
    "        file_path.append(Ravdess + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# changing integers to actual emotions.\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "Ravdess_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "angry       192\n",
       "fear        192\n",
       "disgust     192\n",
       "sad         192\n",
       "surprise    192\n",
       "happy       192\n",
       "calm        192\n",
       "neutral      96\n",
       "Name: Emotions, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take a look at our target variables \n",
    "Ravdess_df['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/Actor_16/03-01-05-01-02-01-16.wav'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ravdess_df['Path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as csv \n",
    "Ravdess_df.to_csv('/Users/stephen/Emotion_Dectection/data/RAVDESS/Ravdess.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset is all balanced except for the \"neutral\" emotion. This doesn't seem like it will be a problem so we'll leave it as is for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the audio data\n",
    "Here we'll use the Pytorch module torchaudio to convert the .wav files to tensor object so we can visualize the signal and then perform the necessary feature extractions to obtain data to train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotions                            angry\n",
       "Path          01/03-01-05-01-02-01-01.wav\n",
       "Intensity                          normal\n",
       "Statement    Dogs are sitting by the door\n",
       "actor                            Actor_01\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declare a variable that will locate a wav file to extract from \n",
    "angry_file_row = Ravdess_df.loc[Ravdess_df['Emotions'] == 'angry']\n",
    "test_file_row = angry_file_row.iloc[0]\n",
    "test_file_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "formats: can't open input file `01/03-01-05-01-02-01-01.wav': No such file or directory\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error loading audio file: failed to open file 01/03-01-05-01-02-01-01.wav",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m3/ywsrz7k170vbk8511l1h_6840000gn/T/ipykernel_84106/3284971015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# load audio file path to tenso\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 filepath, frame_offset, num_frames, normalize, channels_first, format)\n\u001b[1;32m    151\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     return torch.ops.torchaudio.sox_io_load_audio_file(\n\u001b[0m\u001b[1;32m    153\u001b[0m         filepath, frame_offset, num_frames, normalize, channels_first, format)\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error loading audio file: failed to open file 01/03-01-05-01-02-01-01.wav"
     ]
    }
   ],
   "source": [
    "# based on Pytorch Documents \n",
    "# https://pytorch.org/tutorials/beginner/audio_feature_extractions_tutorial.html\n",
    "\n",
    "\n",
    "\n",
    "# global variables \n",
    "n_fft = 2048\n",
    "n_mels = 128\n",
    "n_mfcc = 128\n",
    "win_length = None \n",
    "hop_length = 512\n",
    "sample_rate = None\n",
    "\n",
    "\n",
    "# load audio file path to tenso \n",
    "waveform, sample_rate = torchaudio.load(test_file_row['Path'])\n",
    "\n",
    "print_stats(waveform, sample_rate=sample_rate)\n",
    "plot_waveform(waveform, sample_rate=sample_rate)\n",
    "plot_specgram(waveform, sample_rate)\n",
    "play_audio(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = T.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    ")\n",
    "# Perform transformation\n",
    "spec = spectrogram(waveform)\n",
    "\n",
    "print_stats(spec)\n",
    "plot_spectrogram(spec[0], title='torchaudio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = T.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    onesided=True,\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    ")\n",
    "\n",
    "melspec = mel_spectrogram(waveform)\n",
    "plot_spectrogram(\n",
    "    melspec[0], title=\"MelSpectrogram - torchaudio\", ylabel='mel freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_transform = torchaudio.transforms.MFCC(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mfcc=n_mfcc,\n",
    "    melkwargs={\n",
    "      'n_fft': n_fft,\n",
    "      'n_mels': n_mels,\n",
    "      'hop_length': hop_length,\n",
    "      'mel_scale': 'htk',\n",
    "    })\n",
    "\n",
    "mfcc = mfcc_transform(waveform)\n",
    "\n",
    "plot_spectrogram(mfcc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch = torchaudio.functional.detect_pitch_frequency(waveform, sample_rate)\n",
    "plot_pitch(waveform, sample_rate, pitch)\n",
    "play_audio(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_feature = torchaudio.functional.compute_kaldi_pitch(waveform, sample_rate)\n",
    "pitch, nfcc = pitch_feature[..., 0], pitch_feature[..., 1]\n",
    "\n",
    "plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc)\n",
    "play_audio(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Feature Augmentation \n",
    "https://pytorch.org/tutorials/beginner/audio_feature_augmentation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a Pytorch dataframe odject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# https://www.youtube.com/watch?v=88FFnqt5MNI&ab_channel=ValerioVelardo-TheSoundofAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>/Users/stephen/Emotion_Dectection/data/RAVDESS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                               Path\n",
       "0    angry  /Users/stephen/Emotion_Dectection/data/RAVDESS...\n",
       "1     fear  /Users/stephen/Emotion_Dectection/data/RAVDESS...\n",
       "2     fear  /Users/stephen/Emotion_Dectection/data/RAVDESS...\n",
       "3    angry  /Users/stephen/Emotion_Dectection/data/RAVDESS...\n",
       "4  disgust  /Users/stephen/Emotion_Dectection/data/RAVDESS..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating CSV with labels and path name \n",
    "# https://www.kaggle.com/shivamburnwal/speech-emotion-recognition\n",
    "\n",
    "\n",
    "Ravdess = '/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/'\n",
    "\n",
    "ravdess_directory_list = os.listdir(Ravdess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for dir in ravdess_directory_list:\n",
    "    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
    "    actor = os.listdir(Ravdess + dir)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('-')\n",
    "        # third part in each file represents the emotion associated to that file.\n",
    "        file_emotion.append(int(part[2]))\n",
    "        file_path.append(Ravdess + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# changing integers to actual emotions.\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "Ravdess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "angry       192\n",
       "fear        192\n",
       "disgust     192\n",
       "sad         192\n",
       "surprise    192\n",
       "happy       192\n",
       "calm        192\n",
       "neutral      96\n",
       "Name: Emotions, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ravdess_df['Emotions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3358073929bd3d27ff594bc6528257efc4213b34ba9d7c6bd240dce3a23a83d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('torch_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
