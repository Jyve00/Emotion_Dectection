{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%matplotlib_inline` not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os \n",
    "import sys \n",
    "import torch \n",
    "import torchaudio \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib_inline\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploritory Analysis \n",
    "\n",
    "The RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) is a widely used dataset for emotion classification using recorded speech because of its high quality and consistent audio quality. The dataset can be found at https://smartlaboratory.org/ravdess/ and more info can be found in this offical citation from the creators: \n",
    "\n",
    "Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
    "\n",
    "RAVDESS contains both audio and video but for this project i will only be using and discussions the audio-only portion of the dataset. The database contains audio from 24 actors (12 male, 12 female) each speaking 2 similar sentences in a neutral North American accent. Each statement is spoken in 8 different emotions/ expressions (calm, happy, sad, angry, fearful, suprise, and disgust). Each one is performed in 2 different levels of emotional intensity (normal, strong) and a neutral expression is added. All audio recordings have a sample rate of 48kHz with a bit depth of 16bit. There is a total of 1440 audio files (24 actors X 60 trials per actor).\n",
    "\n",
    "\n",
    "RAVDESS does not come with any sort of metadata table with information on the recordings but instead the filename themselves have all the information. Each filename has a 7 part numerical identifier (ex. 03-01-04-01-01-02-12.wav). The identifiers represent the following: \n",
    "\n",
    "    1.) Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "    2.) Vocal channel (01 = speech, 02 = song).\n",
    "    3.) Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "    4.) Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
    "    5.) Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
    "    6.) Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "    7.) Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "So for example the file 03-01-04-01-01-02-12.wav contains the following metadata: \n",
    "\n",
    "    1.) Audio-only (03)\n",
    "    2.) Speech (01)\n",
    "    3.) Sad (04)\n",
    "    4.) Normal Intensity (01)\n",
    "    5.) \"Kids are talking by the door\" (01)\n",
    "    6.) Second Repetition (02)\n",
    "    7.) Actor-12 Male (12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Metadata table\n",
    "\n",
    "To make the audio data easier to deal with I will create a Pandas data frame that will contain the file path of each audio file and linked to it's emotion as it will be are target variable. The audio files are each separated into their own folders by which actor performed them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m3/ywsrz7k170vbk8511l1h_6840000gn/T/ipykernel_54512/2154891145.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# third part in each file represents the emotion associated to that file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# path to master folder containing \n",
    "audio_data = '/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/'\n",
    "ravdess_directory_list = os.listdir(audio_data)\n",
    "\n",
    "# declare lists that will be used as columns for a pandas dataframe\n",
    "emotion = []\n",
    "path = []\n",
    "intensity = []\n",
    "statement = []\n",
    "actor = []\n",
    "gender = []\n",
    "for dir in ravdess_directory_list:\n",
    "    # as their are 24 different actors in our previous directory we need to extract files for each actor.\n",
    "    actor = os.listdir(audio_data + dir)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('-')\n",
    "        # third part in each file represents the emotion associated to that file.\n",
    "        emotion.append(int(part[2]))\n",
    "        path.append(audio_data + dir + '/' + file)\n",
    "        intent = int(part[3])\n",
    "        if intent == 1:\n",
    "            intent = \"normal\"\n",
    "        else: \n",
    "            intent = \"intense\"\n",
    "        intensity.append(intent)\n",
    "        state = int(part[4])\n",
    "        if state == 1:\n",
    "            state = \"Kids are talking by the door\"\n",
    "        else: \n",
    "            state = \"Dogs are sitting by the door\"\n",
    "        statement.append(state)\n",
    "        act = int(part[6])\n",
    "        actor.append(act)\n",
    "        gen = int(part[6])\n",
    "        if gen%2 == 0:\n",
    "            gen = \"female\"\n",
    "        else: \n",
    "            gen = \"male\"\n",
    "        gender.append(gen)\n",
    "\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for emotional intensity \n",
    "intensity_df = pd.DataFrame(intensity, columns=['Intensity'])\n",
    "\n",
    "# dateframe for statements\n",
    "statement_df = pd.DataFrame(statement, columns=['Statement'])\n",
    "\n",
    "# dataframe for actor number \n",
    "actor_df = pd.DataFrame(actor, columns=['actor'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df, intensity_df, statement_df, actor_df], axis=1)\n",
    "\n",
    "# changing integers to actual emotions.\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "Ravdess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3358073929bd3d27ff594bc6528257efc4213b34ba9d7c6bd240dce3a23a83d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('torch_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
