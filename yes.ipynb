{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=88FFnqt5MNI&ab_channel=ValerioVelardo-TheSoundofAI\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Emotion_speech' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m3/ywsrz7k170vbk8511l1h_6840000gn/T/ipykernel_39752/3388082195.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0memo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmotion_speech\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0memo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mANNOTATIONS_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAUDIO_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There are {len(emo)} samples in the dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Emotion_speech' object is not callable"
     ]
    }
   ],
   "source": [
    "class Emotion_speech(Dataset):\n",
    "    \n",
    "\n",
    "    def __inti__(self, annotations_file, audio_dir):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir \n",
    "\n",
    "    def __len__(self):\n",
    "        return len (self.annotations)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        return signal, label\n",
    "    \n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 1]}\"\n",
    "        path = os.path(fold)\n",
    "        return path \n",
    "        \n",
    "    def _get_audio_sample_labels(self, index):\n",
    "        return self.annotations.iloc[index, 0]\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    ANNOTATIONS_FILE = '/Users/stephen/Emotion_Dectection/data/RAVDESS/metadata.csv'\n",
    "    AUDIO_DIR = '/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/'\n",
    "\n",
    "    emo = Emotion_speech()\n",
    "    emo(ANNOTATIONS_FILE, AUDIO_DIR)\n",
    "\n",
    "    print(f\"There are {len(emo)} samples in the dataset.\")\n",
    "\n",
    "    signal, label = emo[0]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1440 samples in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "class EmotionSpeechDataset(Dataset):\n",
    "\n",
    "    def __init__(self, annotations_file, audio_dir):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        return signal, label\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        #fold = f\"fold{self.annotations.iloc[index, 1]}\"\n",
    "        path = self.annotations.iloc[index, 2]\n",
    "        return path \n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 1]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ANNOTATIONS_FILE = '/Users/stephen/Emotion_Dectection/data/RAVDESS/metadata.csv'\n",
    "    AUDIO_DIR = '/Users/stephen/Emotion_Dectection/data/RAVDESS/Audio_Speech_Actors_01-24/'\n",
    "    emo = EmotionSpeechDataset(ANNOTATIONS_FILE, AUDIO_DIR)\n",
    "    print(f\"There are {len(emo)} samples in the dataset.\")\n",
    "    signal, label = emo[0]\n",
    "\n",
    "    a = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EmotionSpeechDataset at 0x7fa177570280>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3358073929bd3d27ff594bc6528257efc4213b34ba9d7c6bd240dce3a23a83d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('torch_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
